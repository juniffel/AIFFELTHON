{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bc5f420",
   "metadata": {},
   "source": [
    "## â¤ ëª¨ë“ˆ ë¶ˆëŸ¬ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f9ad92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re \n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19abfbf3",
   "metadata": {},
   "source": [
    "## ğŸ§¡ ë°ì´í„° ì½ì–´ì˜¤ê¸°\n",
    "- glob ëª¨ë“ˆì„ ì‚¬ìš©í•˜ë©´ íŒŒì¼ì„ ì½ì–´ì˜¤ëŠ” ì‘ì—…ì„ í•˜ê¸°ê°€ ì•„ì£¼ ìš©ì´í•´ìš”. glob ë¥¼ í™œìš©í•˜ì—¬ ëª¨ë“  txt íŒŒì¼ì„ ì½ì–´ì˜¨ í›„, raw_corpus ë¦¬ìŠ¤íŠ¸ì— ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì €ì¥í•˜ë„ë¡ í• ê²Œìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d7ee980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° í¬ê¸°: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# ì—¬ëŸ¬ê°œì˜ txt íŒŒì¼ì„ ëª¨ë‘ ì½ì–´ì„œ raw_corpus ì— ë‹´ìŠµë‹ˆë‹¤.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"ë°ì´í„° í¬ê¸°:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256fa7a7",
   "metadata": {},
   "source": [
    "## ğŸ’› ë°ì´í„° ì •ì œ\n",
    "- preprocess_sentence() í•¨ìˆ˜ë¥¼ ë§Œë“  ê²ƒì„ ê¸°ì–µí•˜ì‹œì£ ? ì´ë¥¼ í™œìš©í•´ ë°ì´í„°ë¥¼ ì •ì œí•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "- ì¶”ê°€ë¡œ ì§€ë‚˜ì¹˜ê²Œ ê¸´ ë¬¸ì¥ì€ ë‹¤ë¥¸ ë°ì´í„°ë“¤ì´ ê³¼ë„í•œ Paddingì„ ê°–ê²Œ í•˜ë¯€ë¡œ ì œê±°í•©ë‹ˆë‹¤. ë„ˆë¬´ ê¸´ ë¬¸ì¥ì€ ë…¸ë˜ ê°€ì‚¬ ì‘ì‚¬í•˜ê¸°ì— ì–´ìš¸ë¦¬ì§€ ì•Šì„ ìˆ˜ë„ ìˆê² ì£ .\n",
    "ê·¸ë˜ì„œ ì´ë²ˆì—ëŠ” ë¬¸ì¥ì„ í† í°í™” í–ˆì„ ë•Œ í† í°ì˜ ê°œìˆ˜ê°€ 15ê°œë¥¼ ë„˜ì–´ê°€ëŠ” ë¬¸ì¥ì„ í•™ìŠµ ë°ì´í„°ì—ì„œ ì œì™¸í•˜ê¸°ë¥¼ ê¶Œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa99461a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,Â¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,Â¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "# ì´ ë¬¸ì¥ì´ ì–´ë–»ê²Œ í•„í„°ë§ë˜ëŠ”ì§€ í™•ì¸í•´ ë³´ì„¸ìš”.\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9f0230a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ë°ì´í„° ìˆ˜ : 175749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì—¬ê¸°ì— ì •ì œëœ ë¬¸ì¥ì„ ëª¨ì„ê²ë‹ˆë‹¤\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # ìš°ë¦¬ê°€ ì›í•˜ì§€ ì•ŠëŠ” ë¬¸ì¥ì€ ê±´ë„ˆëœë‹ˆë‹¤\n",
    "    if (len(sentence) == 0): continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    # ì •ì œë¥¼ í•˜ê³  ë‹´ì•„ì£¼ì„¸ìš”\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "\n",
    "print(f'ì´ ë°ì´í„° ìˆ˜ : {len(corpus)}')\n",
    "\n",
    "# ì •ì œëœ ê²°ê³¼ë¥¼ 10ê°œë§Œ í™•ì¸í•´ë³´ì£ \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a39626da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus[0].split()) # 0ë²ˆì§¸ ë¬¸ì¥ì˜ ë‹¨ì–´ ê°œìˆ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae2f18",
   "metadata": {},
   "source": [
    "## ğŸ’š í‰ê°€ ë°ì´í„°ì…‹ ë¶„ë¦¬\n",
    "- í›ˆë ¨ ë°ì´í„°ì™€ í‰ê°€ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ì„¸ìš”!\n",
    "\n",
    "- tokenize() í•¨ìˆ˜ë¡œ ë°ì´í„°ë¥¼ Tensorë¡œ ë³€í™˜í•œ í›„, sklearn ëª¨ë“ˆì˜ train_test_split() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ í›ˆë ¨ ë°ì´í„°ì™€ í‰ê°€ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ë‹¨ì–´ì¥ì˜ í¬ê¸°ëŠ” 12,000 ì´ìƒ ìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”! ì´ ë°ì´í„°ì˜ 20% ë¥¼ í‰ê°€ ë°ì´í„°ì…‹ìœ¼ë¡œ ì‚¬ìš©í•´ ì£¼ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52dd973",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2639 ...    0    0    0]\n",
      " [   2   36    7 ...    3    0    0]\n",
      " ...\n",
      " [ 130    5   22 ...   10 1013    3]\n",
      " [   5   37   15 ...  877  647    3]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7f4b6dce7af0>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    # 12000ë‹¨ì–´ë¥¼ ê¸°ì–µí•  ìˆ˜ ìˆëŠ” tokenizerë¥¼ ë§Œë“¤ê²ë‹ˆë‹¤\n",
    "    # ìš°ë¦¬ëŠ” ì´ë¯¸ ë¬¸ì¥ì„ ì •ì œí–ˆìœ¼ë‹ˆ filtersê°€ í•„ìš”ì—†ì–´ìš”\n",
    "    # 12000ë‹¨ì–´ì— í¬í•¨ë˜ì§€ ëª»í•œ ë‹¨ì–´ëŠ” '<unk>'ë¡œ ë°”ê¿€ê±°ì—ìš”\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpusë¥¼ ì´ìš©í•´ tokenizer ë‚´ë¶€ì˜ ë‹¨ì–´ì¥ì„ ì™„ì„±í•©ë‹ˆë‹¤\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # ì¤€ë¹„í•œ tokenizerë¥¼ ì´ìš©í•´ corpusë¥¼ Tensorë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # ì…ë ¥ ë°ì´í„°ì˜ ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì¼ì •í•˜ê²Œ ë§ì¶°ì¤ë‹ˆë‹¤\n",
    "    # ë§Œì•½ ì‹œí€€ìŠ¤ê°€ ì§§ë‹¤ë©´ ë¬¸ì¥ ë’¤ì— íŒ¨ë”©ì„ ë¶™ì—¬ ê¸¸ì´ë¥¼ ë§ì¶°ì¤ë‹ˆë‹¤.\n",
    "    # í† í°ìˆ˜ë¥¼ 15ê°œ ì´í•˜ë¡œ ì œí•œí•©ë‹ˆë‹¤.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post',maxlen=16)\n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92199728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5   91  297   65   57    9  969 6042    3    0    0    0\n",
      "     0    0]\n",
      " [   2   17 2639  873    4    8   11 6043    6  329    3    0    0    0\n",
      "     0    0]\n",
      " [   2   36    7   37   15  164  282   28  299    4   47    7   43    3\n",
      "     0    0]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9e428b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word: # ë‹¨ì–´ í™•ì¸\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dd4a995",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    5   91  297   65   57    9  969 6042    3    0    0    0\n",
      "    0]\n",
      "[  50    5   91  297   65   57    9  969 6042    3    0    0    0    0\n",
      "    0]\n"
     ]
    }
   ],
   "source": [
    "# tensorì—ì„œ ë§ˆì§€ë§‰ í† í°ì„ ì˜ë¼ë‚´ì„œ ì†ŒìŠ¤ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤\n",
    "# ë§ˆì§€ë§‰ í† í°ì€ <end>ê°€ ì•„ë‹ˆë¼ <pad>ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.\n",
    "src_input = tensor[:, :-1]  \n",
    "# tensorì—ì„œ <start>ë¥¼ ì˜ë¼ë‚´ì„œ íƒ€ê²Ÿ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "tgt_input = tensor[:, 1:]    \n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30772f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(175749, 175749)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(src_input),len(tgt_input) # ë°ì´í„° í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6481180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ë¶„ë¦¬\n",
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(\n",
    "                                                    src_input,\n",
    "                                                    tgt_input,\n",
    "                                                    test_size=0.2, \n",
    "                                                    shuffle= True,\n",
    "                                                    random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e556e84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((140599, 15), (35150, 15), (140599, 15), (35150, 15))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_train.shape, enc_val.shape, dec_train.shape, dec_val.shape # í›ˆë ¨ë°ì´í„°, í…ŒìŠ¤íŠ¸ë°ì´í„° ëª¨ì–‘\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a5219c",
   "metadata": {},
   "source": [
    "## ğŸ’™ ì¸ê³µì§€ëŠ¥ ë§Œë“¤ê¸°\n",
    "- ëª¨ë¸ì˜ Embedding Sizeì™€ Hidden Sizeë¥¼ ì¡°ì ˆí•˜ë©° 10 Epoch ì•ˆì— val_loss ê°’ì„ 2.2 ìˆ˜ì¤€ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆëŠ” ëª¨ë¸ì„ ì„¤ê³„í•˜ì„¸ìš”!\n",
    "\n",
    "- ì˜ ì„¤ê³„í•œ ëª¨ë¸ì„ í•™ìŠµí•˜ë ¤ë©´, model.fit() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. model.fit() í•¨ìˆ˜ì—ëŠ” ë‹¤ì–‘í•œ ì¸ìë¥¼ ë„£ì–´ì£¼ì–´ì•¼ í•˜ëŠ”ë°, ê°€ì¥ ê¸°ë³¸ì ì¸ ì¸ìë¡œëŠ” ë°ì´í„°ì…‹ê³¼ epochsê°€ ìˆìŠµë‹ˆë‹¤. '5. ì‹¤ìŠµ (2) ì¸ê³µì§€ëŠ¥ í•™ìŠµì‹œí‚¤ê¸°'ì—ì„œì˜ ì˜ˆì‹œì™€ ê°™ì´ ë§ì´ì£ .\n",
    "\n",
    "- model.fit(dataset, epochs=30)\n",
    "- í•˜ì§€ë§Œ model.fit() í•¨ìˆ˜ì˜ epochsë¥¼ ì•„ë¬´ë¦¬ í¬ê²Œ ë„£ëŠ”ë‹¤ í•´ë„ val_loss ê°’ì€ 2.2 ì•„ë˜ë¡œ ë–¨ì–´ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ëŸ´ ê²½ìš°ëŠ” batch sizeë¥¼ ë³€ê²½í•˜ëŠ” ê²ƒê³¼ ê°™ì´ model.fit() í•¨ìˆ˜ì— ë‹¤ì–‘í•œ ì¸ìë¥¼ ë„£ì–´ì£¼ë©´ í•´ê²°ë  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit ë¥¼ ì°¸ê³ í•˜ì„¸ìš”!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bb371b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 2000\n",
    "hidden_size = 1900\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a63a464f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "4394/4394 [==============================] - 574s 127ms/step - loss: 2.8718 - val_loss: 2.5794\n",
      "Epoch 2/4\n",
      "4394/4394 [==============================] - 563s 128ms/step - loss: 2.3019 - val_loss: 2.3430\n",
      "Epoch 3/4\n",
      "4394/4394 [==============================] - 563s 128ms/step - loss: 1.8792 - val_loss: 2.2310\n",
      "Epoch 4/4\n",
      "4394/4394 [==============================] - 564s 128ms/step - loss: 1.5614 - val_loss: 2.1991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4b5c1f22b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "#Loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "0\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "\n",
    "model.fit(enc_train,dec_train, epochs=4,validation_data=(enc_val, dec_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3190954c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  24002000  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  29647600  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  28887600  \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  22813901  \n",
      "=================================================================\n",
      "Total params: 105,351,101\n",
      "Trainable params: 105,351,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e05ec",
   "metadata": {},
   "source": [
    "## ğŸ’œ ë¬¸ì¥ìƒì„± "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd8a4332",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ì„œ ì…ë ¥ë°›ì€ init_sentenceë„ í…ì„œë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    # ë‹¨ì–´ í•˜ë‚˜ì”© ì˜ˆì¸¡í•´ ë¬¸ì¥ì„ ë§Œë“­ë‹ˆë‹¤\n",
    "    #    1. ì…ë ¥ë°›ì€ ë¬¸ì¥ì˜ í…ì„œë¥¼ ì…ë ¥í•©ë‹ˆë‹¤\n",
    "    #    2. ì˜ˆì¸¡ëœ ê°’ ì¤‘ ê°€ì¥ ë†’ì€ í™•ë¥ ì¸ word indexë¥¼ ë½‘ì•„ëƒ…ë‹ˆë‹¤\n",
    "    #    3. 2ì—ì„œ ì˜ˆì¸¡ëœ word indexë¥¼ ë¬¸ì¥ ë’¤ì— ë¶™ì…ë‹ˆë‹¤\n",
    "    #    4. ëª¨ë¸ì´ <end>ë¥¼ ì˜ˆì¸¡í–ˆê±°ë‚˜, max_lenì— ë„ë‹¬í–ˆë‹¤ë©´ ë¬¸ì¥ ìƒì„±ì„ ë§ˆì¹©ë‹ˆë‹¤\n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor) \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        # 3 \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    # tokenizerë¥¼ ì´ìš©í•´ word indexë¥¼ ë‹¨ì–´ë¡œ í•˜ë‚˜ì”© ë³€í™˜í•©ë‹ˆë‹¤ \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ac0ad15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love the way you lie shine bright like a diamond <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e763d6",
   "metadata": {},
   "source": [
    "## ğŸ¤ íšŒê³ \n",
    "\n",
    "### ì „ì²´ê³¼ì • ì •ë¦¬\n",
    "1. ë°ì´í„° ì •ì œ\n",
    "    - ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©\n",
    "    - ì „ì²˜ë¦¬\n",
    "2. í…ì„œ ë³€í™˜\n",
    "    - ë°ì´í„°ë¥¼ í† í°í™” í•˜ê³  í…ì„œë¡œ ë³€í™˜ \n",
    "    - ì´ê³¼ì •ì—ì„œ maxlenì„ ì‚¬ìš©í•˜ì—¬ ìµœëŒ€ í† í°ê°œìˆ˜ë¥¼ 15ê°œ ì´í•˜ë¡œ ì œí•œí•˜ì˜€ë‹¤.\n",
    "3. ë°ì´í„° ë¶„ë¦¬ \n",
    "    - í•™ìŠµê³¼ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•´ ì£¼ì—ˆë‹¤.\n",
    "4. ëª¨ë¸ í•™ìŠµ \n",
    "    - í•˜ì´í¼ íŒŒë¦¬ë¯¸í„° ì¡°ì •ì„ í†µí•´ val_lossê°€ 2.2 ì´í•˜ë¡œ ë‚˜ì˜¤ê²Œ ì„¤ì •\n",
    "5. ë¬¸ì¥ìƒì„±\n",
    "    - í•™ìŠµí•œ ê²°ê³¼ë¥¼ í† ëŒ€ë¡œ ì‹¤ì œ ë¬¸ì¥ì„ ìƒì„±í•´ ë³´ì•˜ë‹¤.\n",
    "    \n",
    "### ê³ ë‚œê³¼ ì—­ê²½\n",
    "\n",
    "- val_lossë¥¼ ì¤„ì´ëŠ” ê³¼ì •ì—ì„œ ìƒë‹¹í•œ ì‹œë ¨ì„ ë§›ë´¤ë‹¤. batch_size, embedding_size, hidden_sizeë“± ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•´ ë³´ì•˜ê³  ê·¸ê²°ê³¼ 2,19ë¼ëŠ” ê°’ì§„ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆë‹¤.\n",
    "\n",
    "- ë¬¸ì¥ ì¶œë ¥ê²°ê³¼ í•´ì„í•˜ê¸° ì• ë§¤í•˜ê¸´ í•˜ì§€ë§Œ ë¬¸ì¥ì„ ì˜ í˜•ì„± í•˜ëŠ”ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca20db6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "be04ff51d9b28d0a801579a1b9c0a46c912d4ebd8df3f1a97b27efafcf776c92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
